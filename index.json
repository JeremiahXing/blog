[{"content":"What is awk? awk is a powerful scripting language and command-line utility for text-processing in Unix/Linux environments. It scans each line of a file, looking for patterns that match, and then processes the line accordingly (kinda like reduce in JavaScript but has difference). It\u0026rsquo;s excellent for manipulating structured text data like CSV, logs, or even simple text files.\nBasic Syntax The basic syntax of awk is:\nawk \u0026#39;pattern {action}\u0026#39; file pattern: A condition to match (like regex or conditional expressions). If omitted, awk processes all lines. action: Commands to execute when a pattern matches. Actions are enclosed in {}. Types of Patterns in awk Regular Expressions: Use regular expressions as patterns to match lines. Example: awk '/error/ {print}' file.txt prints lines containing \u0026ldquo;error\u0026rdquo;.\nRelational Expressions: These include comparisons of numbers or strings. Example: awk '$2 \u0026gt; 100 {print}' file.txt prints lines where the second field is greater than 100.\nCompound Patterns: Combine patterns using logical operators (\u0026amp;\u0026amp;, ||, !). Example: awk '$1 == \u0026quot;start\u0026quot; \u0026amp;\u0026amp; $3 \u0026gt; 50 {print}' file.txt prints lines where the first field is \u0026ldquo;start\u0026rdquo; and the third field is over 50.\nRange Patterns: Match lines from the start pattern to the end pattern. Example: awk '/start/, /end/ {print}' file.txt prints lines from \u0026ldquo;start\u0026rdquo; to \u0026ldquo;end\u0026rdquo;.\nSpecial Patterns BEGIN and END: BEGIN: Executed before reading any input lines, often for initialization. END: Executed after all lines are processed, used for summaries or conclusions.\nExample Problem: Word Frequency Let\u0026rsquo;s use LeetCode 192. Word Frequency: write a bash script to calculate the frequency of each word in a text file words.txt as a example.\nThe solution of this problem is straight forward, bascially in 3 steps:\nRead the file words.txt; Walk through each word, record/update the frequencey; Sort and print out the result. However, implementing this in a shell script can be hard, especially for people who aren\u0026rsquo;t familiar with Linux Shell Script. It\u0026rsquo;s even more challenging if you want to write it in a single line of script code (and yeah all Linux developers do it cause it looks cool af XD)\nBut no worries, let\u0026rsquo;s break the damn thing down with Shell Pipe. If you don\u0026rsquo;t know what it is, I have another blog about that and feel free to check it out.\nSolution: Read the File cat words.txt\nThis command reads the content of the file words.txt. cat is short for concatenate. When used with a file name, it displays the content of the file.\necho -e $(...)\nThe $(...) is command substitution, which means it executes the command inside the parentheses and then echo outputs the result. echo -e enables interpretation of backslash escapes. When used with the output of cat words.txt, it converts newlines into spaces. This effectively puts all the words on a single line.\nUpdate frequencey This awk script is the core part of the command.\nfor(i=1; i\u0026lt;=NF; i++){words[$i]++}: This loop iterates over all fields (words) in a line. NF is a built-in variable in awk that represents the number of fields in the current record (line). For each word, it increments the count in the associative array words.\nEND {for(w in words) print w, words[w]}: After processing all lines, this part executes. It iterates through each index (word) in the array words and prints the word and its frequency.\nSort and Print sort -k2 -n -r\nThis sorts the output from the awk command. -k2 tells sort to sort based on the second column (which is the word frequency). -n means to sort numerically (since word counts are numbers). -r sorts in reverse order, so you get a list from the most frequent to the least frequent word.\necho -e $(cat words.txt) | awk \u0026#39;{for(i=1; i\u0026lt;=NF; i++){words[$i]++}} END {for(w in words) print w, words[w]}\u0026#39; | sort -k2 -n -r ","permalink":"https://jeremiahxing.github.io/blog/posts/bash_awk/","summary":"What is awk? awk is a powerful scripting language and command-line utility for text-processing in Unix/Linux environments. It scans each line of a file, looking for patterns that match, and then processes the line accordingly (kinda like reduce in JavaScript but has difference). It\u0026rsquo;s excellent for manipulating structured text data like CSV, logs, or even simple text files.\nBasic Syntax The basic syntax of awk is:\nawk \u0026#39;pattern {action}\u0026#39; file pattern: A condition to match (like regex or conditional expressions).","title":"Understand awk in Linux Shell"},{"content":"Shell pipes are a powerful feature of command-line interfaces in Unix and Unix-like systems, including Linux and macOS. They allow you to pass the output of one command directly into another command, creating a \u0026ldquo;pipeline\u0026rdquo; of operations. This can greatly simplify your tasks and enable efficient data processing. In this post, we\u0026rsquo;ll explore the basics of shell pipes and see some practical examples.\nWhat is a Shell Pipe? A shell pipe, denoted by the | symbol, takes the output (stdout) of one command and uses it as the input (stdin) for the next command. This chaining of commands allows you to perform complex tasks with a combination of simple commands.\nBasic Example of Using a Pipe Consider a situation where you have a file named names.txt containing a list of names, and you want to find out how many names are in the file. You can use a combination of cat, grep, and wc commands:\ncat names.txt | grep -v \u0026#34;^#\u0026#34; | wc -l In this command:\ncat names.txt displays the content of names.txt. grep -v \u0026quot;^#\u0026quot; filters out lines starting with # (comments, for instance). wc -l counts the number of lines. The pipe | passes the output of each command to the next. Pipes are particularly useful for sorting and filtering data. For example, if you want to list all files in the current directory, sorted by file size:\nls -l | sort -nk5 This command:\nls -l lists files in long format. sort -nk5 sorts the output numerically (-n) based on the fifth column (-k5), which is the file size. You can also chain multiple pipes together. Suppose you want to check the most used words in a text file:\ncat article.txt | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | sort | uniq -c | sort -nr | head -10 This breaks down as:\ncat article.txt displays the file content. tr ' ' '\\n' replaces spaces with new lines, putting each word on a separate line. sort sorts the words alphabetically. uniq -c counts the occurrences of each word. sort -nr sorts the word count in descending order. head -10 shows the top 10 results. Conclusion Shell pipes are a simple yet powerful tool for processing and manipulating data on the command line. By understanding and using pipes, you can combine basic commands to perform complex tasks efficiently.\n","permalink":"https://jeremiahxing.github.io/blog/posts/bash_pipe/","summary":"Shell pipes are a powerful feature of command-line interfaces in Unix and Unix-like systems, including Linux and macOS. They allow you to pass the output of one command directly into another command, creating a \u0026ldquo;pipeline\u0026rdquo; of operations. This can greatly simplify your tasks and enable efficient data processing. In this post, we\u0026rsquo;ll explore the basics of shell pipes and see some practical examples.\nWhat is a Shell Pipe? A shell pipe, denoted by the | symbol, takes the output (stdout) of one command and uses it as the input (stdin) for the next command.","title":"Shell Pipes: Simplifying Command Line Tasks"}]